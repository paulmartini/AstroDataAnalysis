{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69f05276-d17d-4335-9da1-4503d1069218",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Class 10: LLM Function Tools and Retrieval Augmented Generation\n",
    "## Objective: Transition from simple chat prompts to structured, tool-using agents\n",
    "\n",
    "An **AI Agent** is a software program that operates autonomously to achieve specific goals. They can solve problems and make decisions without continual human oversight, they often use LLMs to understand context, can connect to external applications (including APIs), and in some cases can retain information from previous interactions and have knowledge about their environment.\n",
    "\n",
    "A **Schema** for a function describes what a function does, what parameters it needs, and when to use it. Schemas are important for LLMs to know when and how to use functions. \n",
    "\n",
    "**Instructions:** Work with one or more students at your table. Discuss the key concepts and the code logic with one another. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558627d7-76fb-4a12-8f5d-069f4dfb7c03",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Check that you have downloaded the `.env` and `.gitignore` files from carmen and put them in the same directory as your class notebooks. If you have not done so, go through Class 9 notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3291c318-cf02-4309-b191-d005f1cb4bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import litellm\n",
    "import base64\n",
    "import json\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Suppress specific Pydantic warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pydantic\")\n",
    "\n",
    "custom_api_base = \"https://litellmproxy.osu-ai.org\" \n",
    "astro1221_key = os.getenv(\"ASTRO1221_API_KEY\")\n",
    "\n",
    "def prompt_llm(messages, model=\"openai/GPT-4.1-mini\", temperature=0.2, max_tokens=1000, tools=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Send a prompt or conversation to an LLM using LiteLLM and return the response.\n",
    "    Exact function from Class 9 notebook.\n",
    "    \"\"\"\n",
    "    if isinstance(messages, str):\n",
    "        messages = [{\"role\": \"user\", \"content\": messages}]\n",
    "    if not (isinstance(temperature, (int, float)) and 0 <= temperature <= 2):\n",
    "        raise ValueError(\"temperature must be a float between 0 and 2 (inclusive).\")\n",
    "    if not (isinstance(max_tokens, int) and max_tokens > 0):\n",
    "        raise ValueError(\"max_tokens must be a positive integer.\")\n",
    "\n",
    "    try: \n",
    "        print(\"Contacting LLM via University Server...\")\n",
    "        response = litellm.completion(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            api_base=custom_api_base,\n",
    "            api_key=astro1221_key,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        answer = response['choices'][0]['message']['content']\n",
    "        if verbose: \n",
    "            print(f\"\\nSUCCESS! Here is the answer from {model}:\\n\")\n",
    "            print(answer)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR: Could not connect. Details:\\n{e}\")    \n",
    "        response = None\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb70d8b0-42ec-4918-856e-1d6eadd4b342",
   "metadata": {},
   "source": [
    "## Section 1: Unified LLM Interfacing\n",
    "\n",
    "**Purpose:** Write unified code that interacts with different LLMs\n",
    "\n",
    "To avoid rewriting code for different providers (OpenAI, Anthropic, Google), we use LiteLLM. This provides a streamlined way to interact with various models using a single syntax. The key to this is **Message Roles:**\n",
    "\n",
    "**System:** Sets the \"persona\" or rules for the AI.\n",
    "\n",
    "**User:** Your specific question or command.\n",
    "\n",
    "**Assistant:** The model's previous responses (used to maintain context)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f9f425-2ceb-45e8-a0c3-7fed147ca693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Creating a specific persona using System roles\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a precise astronomical data validator. Be brief.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Is it possible for a star to have a surface temperature of 50,000K?\"}\n",
    "]\n",
    "\n",
    "response = prompt_llm(messages, model=\"gemini/gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c995d2-a2a3-4efc-9036-49611f1824a9",
   "metadata": {},
   "source": [
    "**Test your understanding:** Modify the message list below so the System role instructs the AI to respond like a \"17th-century astronomer using archaic language,\" and ask the User question: \"What is the nature of the Milky Way?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bf2607-da88-4bad-8f4c-214d8ab0bed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your instructions here:\n",
    "archaic_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Response instructions\"},\n",
    "    {\"role\": \"user\", \"content\": \"Question\"}\n",
    "]\n",
    "# uncomment when ready\n",
    "# response = prompt_llm(archaic_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eb863e-e4b1-4e61-b47a-19975288a671",
   "metadata": {},
   "source": [
    "## Section 2: Structured Outputs and JSON Mode\n",
    "\n",
    "**Purpose:** We can enforce JSON output to make responses easier to parse.  \n",
    "\n",
    "Astronomers often need data that can be programmatically parsed by Python (e.g., a list of coordinates) rather than a conversational paragraph. By instructing the model to use **JSON Enforcement**, we ensure the output is a valid dictionary that can be loaded directly into a data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3065f552-93f4-4fb8-bdc6-348a896af905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using prompt engineering to get structured JSON data\n",
    "prompt = \"Return a JSON object containing the 'constellation', 'brightest_star', and 'approx_distance_ly' for the constellation Lyra. Return ONLY the JSON.\"\n",
    "\n",
    "response = prompt_llm(prompt, temperature=0.1)\n",
    "\n",
    "# Parsing the string output into a Python dictionary\n",
    "if response:\n",
    "    raw_text = response['choices'][0]['message']['content']\n",
    "    # Clean up markdown if the model included it\n",
    "    clean_json = raw_text.replace('```json', '').replace('```', '').strip()\n",
    "    data = json.loads(clean_json)\n",
    "    print(f\"The brightest star is: {data['brightest_star']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2755df97-be49-4d05-97c6-e59b02e605cb",
   "metadata": {},
   "source": [
    "**Test your understanding:** Ask the model for a JSON object containing the name, type (e.g., Spiral, Elliptical), and redshift of the Sombrero Galaxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa596afd-e5c2-4e08-a105-d8f478468122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "json_prompt = \"\"\n",
    "# response = prompt_llm(json_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c897e7-cb1d-49a8-9cdc-20e9513203c6",
   "metadata": {},
   "source": [
    "## Section 3: LLMs in functions \n",
    "\n",
    "One example of the power of the LLM API interface is when we have to repeat large numbers of tasks that would be very time consuming to copy/paste into a chatbot interface on a web browser. \n",
    "\n",
    "Below is a set of astronomical data. The function identifies each object, looks up information about it, and then returns the information in a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a038fa-3768-4958-a1c8-fd33cc62eeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 1. Your raw data (this could also be loaded from a CSV or Text file)\n",
    "astronomical_data = [\n",
    "    \"The star Betelgeuse is a red supergiant located roughly 640 light-years away.\",\n",
    "    \"Sirius A is the brightest star in the night sky and is a main-sequence star.\",\n",
    "    \"M31, also known as the Andromeda Galaxy, is a spiral galaxy 2.5 million light-years from Earth.\",\n",
    "    \"Proxima Centauri is a red dwarf star and the closest star to the Sun at 4.24 light-years.\",\n",
    "    \"Rigel is a blue supergiant in the constellation Orion, situated about 860 light-years from our solar system.\",\n",
    "    \"The Crab Nebula (M1) is a supernova remnant in the constellation Taurus, approximately 6,500 light-years away.\",\n",
    "    \"Sagittarius A* is the supermassive black hole at the center of the Milky Way, located 26,000 light-years from Earth.\",\n",
    "    \"Vega is a bright blue-white main-sequence star in the Lyra constellation, about 25 light-years distant.\",\n",
    "    \"The Sombrero Galaxy (M104) is an unbarred spiral galaxy found 29 million light-years away in Virgo.\",\n",
    "    \"Canopus is a white giant and the second-brightest star in the sky, located 310 light-years from Earth.\",\n",
    "    \"Polaris, the North Star, is a yellow supergiant star system roughly 430 light-years away.\",\n",
    "    \"Alpha Centauri A is a yellow main-sequence star, similar to the Sun, part of a system 4.37 light-years away.\",\n",
    "    \"The Pleiades (M45) is an open star cluster in Taurus containing middle-aged, hot B-type stars about 444 light-years distant.\",\n",
    "    \"Arcturus is a red giant star in the constellation Boötes and is the fourth-brightest star, 37 light-years away.\",\n",
    "    \"The Whirlpool Galaxy (M51) is a classic spiral galaxy located 23 million light-years from Earth in Canes Venatici.\"\n",
    "]\n",
    "# 2. A dictionary to store the processed results\n",
    "processed_catalog = {}\n",
    "\n",
    "print(\"Starting data processing...\")\n",
    "\n",
    "for entry in astronomical_data:\n",
    "    # Create a prompt that enforces JSON output for easy parsing\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following text and extract the object's name, its classification, \n",
    "    and its approximate distance.\n",
    "    \n",
    "    TEXT: {entry}\n",
    "    \n",
    "    Return ONLY a JSON object with the keys: 'name', 'type', and 'distance'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Call the prompt_llm() function\n",
    "    # We use a low temperature for more consistent, factual data\n",
    "    response = prompt_llm(prompt, temperature=0.1, verbose=False)\n",
    "    \n",
    "    if response:\n",
    "        try:\n",
    "            # Extract the raw string from the response\n",
    "            raw_content = response['choices'][0]['message']['content']\n",
    "            \n",
    "            # Clean Markdown formatting if the model included it\n",
    "            clean_json = raw_content.replace('```json', '').replace('```', '').strip()\n",
    "            \n",
    "            # Convert the JSON string into a Python dictionary\n",
    "            object_info = json.loads(clean_json)\n",
    "            \n",
    "            # Use the object name as the key in our main catalog dictionary\n",
    "            name = object_info['name']\n",
    "            processed_catalog[name] = {\n",
    "                \"type\": object_info['type'],\n",
    "                \"distance\": object_info['distance']\n",
    "            }\n",
    "            print(f\"Successfully processed: {name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing data for entry: {entry}. Error: {e}\")\n",
    "\n",
    "# 3. View the final structured results\n",
    "print(\"\\nFinal Processed Catalog (stored in processed_catalog):\")\n",
    "print(json.dumps(processed_catalog, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5cd0af-b9e4-47b3-93e5-7a2b4fee4947",
   "metadata": {},
   "source": [
    "**Test your understanding:** Write a loop to print the distance and type for each object in `processed_catalog`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c95c02-1170-4db9-ab80-4071d86c1497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027d2a5c-c877-4836-9e93-5b528d43da70",
   "metadata": {},
   "source": [
    "## Section 4: Function Calling (Tool Use)\n",
    "\n",
    "Function tools allow LLMs to call your functions. This is valuable because you control how a calculation or other action is performed, which can be both more accurate and more efficient that to use an LLM. \n",
    "\n",
    "You provide a function and a `JSON Schema` to the LLM that describes what the function does and what arguments it needs. The model then decides if it needs to call that function to answer a question. You can provide many functions, with a schema for each.\n",
    "\n",
    "Here is an example with simple function to compute distance from parallax, along with the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef72761-d7c2-4b5e-b217-a2181b3dbb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallax_to_distance(parallax_arcsec):\n",
    "    \"\"\"\n",
    "    Convert stellar parallax to distance in parsecs.\n",
    "    \n",
    "    The fundamental equation: d = 1/p\n",
    "    where d is distance in parsecs and p is parallax in arcseconds.\n",
    "    \"\"\"\n",
    "    # Input validation - always check for invalid inputs!\n",
    "    if parallax_arcsec <= 0:\n",
    "        return {\"error\": \"Parallax must be positive\"}\n",
    "    \n",
    "    # Calculate distance using the parallax formula\n",
    "    distance_pc = 1.0 / parallax_arcsec\n",
    "    \n",
    "    # Return as a dictionary for structured data\n",
    "    # We round to 2 decimal places for readability\n",
    "    return {\"distance_parsecs\": round(distance_pc, 2)}\n",
    "\n",
    "# Create a tools list with our function schema\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"parallax_to_distance\",\n",
    "            \"description\": \"Calculate the distance to a star in parsecs given its parallax in arcseconds.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"parallax_arcsec\": {\n",
    "                        \"type\": \"number\", \n",
    "                        \"description\": \"The parallax value in arcseconds (e.g., 0.742 for Alpha Centauri)\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"parallax_arcsec\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Function schema defined\")\n",
    "print(f\"The LLM knows about {len(tools)} function(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15858023-b6b2-4b52-baba-9ae2174f86d8",
   "metadata": {},
   "source": [
    "### Illustration:\n",
    "\n",
    "Here is an example interaction where the LLM is told about the tool and uses it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b62586c-0281-4eed-8d75-7e2e2331c009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: User asks a question requiring a calculation\n",
    "messages = [{\"role\": \"user\", \"content\": \"The star Proxima Centauri has a parallax of 0.768 arcseconds. How far away is it in parsecs?\"}]\n",
    "\n",
    "# Step 2: Call the LLM with the tools schema defined\n",
    "response = prompt_llm(messages, tools=tools) \n",
    "\n",
    "message = response.choices[0].message\n",
    "\n",
    "# Step 3: Check if the model wants to call a tool\n",
    "if message.tool_calls:\n",
    "    tool_call = message.tool_calls[0]\n",
    "    function_name = tool_call.function.name\n",
    "    \n",
    "    if function_name == \"parallax_to_distance\":\n",
    "        # Extract the arguments the LLM decided to use\n",
    "        args = json.loads(tool_call.function.arguments)\n",
    "        p_val = args.get(\"parallax_arcsec\")\n",
    "        \n",
    "        print(f\"Agent Thought: I need to calculate distance for p={p_val}\")\n",
    "        \n",
    "        # Step 4: Execute the actual Python function locally\n",
    "        observation = parallax_to_distance(p_val)\n",
    "        print(f\"Observation from Tool: {observation}\")\n",
    "        \n",
    "        # Step 5: Feed the observation back to the LLM to get the final answer\n",
    "        messages.append(message) # Add the model's tool call to history\n",
    "        messages.append({\n",
    "            \"role\": \"tool\",\n",
    "            \"tool_call_id\": tool_call.id,\n",
    "            \"name\": function_name,\n",
    "            \"content\": json.dumps(observation)\n",
    "        })\n",
    "        \n",
    "        final_response = prompt_llm(messages, tools=tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2d48df-74b2-4c7f-bff5-0ea0a642d332",
   "metadata": {},
   "source": [
    "### Add another function and schema \n",
    "\n",
    "The next cell has a function `stellar_luminosity()` to compute a star's luminosity based on its size and temperature and adds the schema for this function to `tools`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b097ce2a-e30d-4fde-b6e4-8fe9471d797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from\n",
    "# https://tingyuansen.github.io/coding_essential_for_astronomers/lectures/lecture08-llm-function-tools-and-rag.html\n",
    "\n",
    "def stellar_luminosity(radius_solar, temperature_k):\n",
    "    \"\"\"\n",
    "    Calculate stellar luminosity using the Stefan-Boltzmann law.\n",
    "    \n",
    "    The energy radiated by a star depends on its surface area (4πR²)\n",
    "    and how much energy each square meter emits (σT⁴).\n",
    "    \"\"\"\n",
    "    # Physical constants\n",
    "    stefan_boltzmann = 5.67e-8  # W m^-2 K^-4 (Stefan-Boltzmann constant)\n",
    "    solar_radius = 6.96e8  # meters (Sun's radius)\n",
    "    solar_luminosity = 3.83e26  # watts (Sun's total energy output)\n",
    "    \n",
    "    # Always validate inputs\n",
    "    if radius_solar <= 0 or temperature_k <= 0:\n",
    "        return {\"error\": \"Radius and temperature must be positive\"}\n",
    "    \n",
    "    # Convert stellar radius from solar units to meters\n",
    "    radius_meters = radius_solar * solar_radius\n",
    "    \n",
    "    # Apply Stefan-Boltzmann law: L = 4πR²σT⁴\n",
    "    luminosity_watts = 4 * np.pi * radius_meters**2 * stefan_boltzmann * temperature_k**4\n",
    "    \n",
    "    # Convert to solar luminosities for easier interpretation\n",
    "    luminosity_solar = luminosity_watts / solar_luminosity\n",
    "    \n",
    "    return {\n",
    "        \"luminosity_solar\": round(luminosity_solar, 3),\n",
    "        \"luminosity_watts\": f\"{luminosity_watts:.2e}\"  # Scientific notation\n",
    "    }\n",
    "\n",
    "# Define the schema for the luminosity function\n",
    "luminosity_schema = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"stellar_luminosity\",\n",
    "        \"description\": \"Calculate stellar luminosity using the Stefan-Boltzmann law ($L = 4\\pi R^2 \\sigma T^4$).\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"radius_solar\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"The radius of the star in units of solar radii (R_sun).\"\n",
    "                },\n",
    "                \"temperature_k\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"The effective surface temperature of the star in Kelvin (K).\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"radius_solar\", \"temperature_k\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Append it to your existing tools list\n",
    "tools.append(luminosity_schema)\n",
    "print(f\"The LLM knows about {len(tools)} function(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6168e4-e653-4d45-b6c7-3c30e1b85a1e",
   "metadata": {},
   "source": [
    "**Test your understanding:** Add code to the following code cell to test the stellar_luminosity() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc03f42-8dcd-4ff3-b0bb-8c5ba871c074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331f893a",
   "metadata": {},
   "source": [
    "## Section 5: Context Windows & The Knowledge Gap\n",
    "\n",
    "LLMs have a **knowledge cut-off** (the date their training ended). They are also limited by their **Context Window** (how much text they can \"read\" at once). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c663c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a 'Knowledge Gap' question\n",
    "prompt = \"What were the main findings of the Astro-Deep-Search paper published in December 2025?\"\n",
    "print(prompt) \n",
    "print(\"Without RAG, the model will likely say it doesn't know or will hallucinate a guess.\")\n",
    "response = prompt_llm(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67c3ed4",
   "metadata": {},
   "source": [
    "**Test your understanding:** Ask this question by copying it ito the prompt in the next cell: \"If an LLM was trained in early 2024, can it correctly identify the current orbital position of a comet discovered in 2025 without using RAG or a Tool?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ecd342",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\n",
    "response = prompt_llm(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5b7285-b1ef-4b9c-b073-ae3117a2b0a7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Section 6: Retrieval Augmented Generation (RAG)\n",
    "\n",
    "RAG solves the problem of hallucinations and the \"Knowledge Gap.\" Instead of relying on the model's internal memory (which might be outdated), we provide a \"cheat sheet\" of relevant text snippets (the Context) for the model to read before it answers.\n",
    "\n",
    "RAG is essential when you are working with data or papers released after the model was trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d39c6c-f0d2-4e4b-8c7a-75d836b2a78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'Retrieval' part: We find a relevant snippet from a recent paper\n",
    "print(\"\\n** Include context in the query\")\n",
    "context_snippet = \"The 2025 survey of the G-79 cluster found 12 new blue stragglers.\"\n",
    "\n",
    "query = \"How many blue stragglers were found in the G-79 cluster survey?\"\n",
    "\n",
    "# The 'Generation' part: We combine context + query\n",
    "rag_prompt = f\"Use this context to answer: {context_snippet}\\n\\nQuestion: {query}\"\n",
    "\n",
    "response = prompt_llm(rag_prompt)\n",
    "\n",
    "print('=' * 60)\n",
    "print(\"\\n** Compare to the result without the context\")\n",
    "\n",
    "response = prompt_llm(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb62004-8150-4dc7-9175-4185acc7ff50",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Section 7: RAG with a pre-existing database\n",
    "\n",
    "I have created a database with all of the lecture content from the online textbook. You can download this database and then use the code below to query this database. This query will also return which lecture has the material. \n",
    "\n",
    "### Instructions to download the database\n",
    "\n",
    "1. Download the database file `Astro1221-LectureDB.zip` from carmen\n",
    "2. Unzip this file on your computer. I recommend you do this in the same directory structure you use for the class notebooks. For example, if you have your notebooks in a directory like `Astro1221/Notebooks`, I recommend you put the database file in `Astro1221/` and unzip it there. This will create a directory called `Astro1221-LectureDB/`. \n",
    "3. Check that the `Astro1221-LectureDB/` directory contains a `chroma.sqlite3` file and a subdirectory with binary files.\n",
    "\n",
    "### Environmental Setup\n",
    "\n",
    "Make sure you have the necessary libraries to use the database. Run this command in your terminal:\n",
    "\n",
    "`python -m pip install chromadb sentence-transformers`\n",
    "\n",
    "The next code block defines the database query. Edit the `DB_PATH` to point to the database on your computer, then test the agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207c8720-06dd-4ea1-a824-a8f8c893cb21",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# --- SETUP ---\n",
    "# 1. Change DB_PATH to point to the folder where you unzipped Astro1221-LectureDB.zip\n",
    "DB_PATH = \"/Users/martini.10/OneDrive - The Ohio State University/Teaching/Astro1221/Sp26/Astro1221-LectureDB\"\n",
    "COLLECTION_NAME = \"Astro1221\"\n",
    "\n",
    "# 2. Load the same embedding model used to create the database\n",
    "# This is required so the 'math' of the search matches the database\n",
    "print(\"Loading brain (embedding model)...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# 3. Connect to the shared database\n",
    "client = chromadb.PersistentClient(path=DB_PATH)\n",
    "collection = client.get_collection(name=COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878fb2fa-f2dd-4846-90e8-1e773fb3db5d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ask_astronomy_ai(query):\n",
    "    # 1. Retrieve the top 3 most relevant snippets\n",
    "    # The 'results' object contains 'documents' and 'metadatas'\n",
    "    results = collection.query(query_texts=[query], n_results=3)\n",
    "    \n",
    "    # 2. Extract the text and the sources\n",
    "    chunks = results['documents'][0]\n",
    "    sources = results['metadatas'][0] # This contains [{\"source\": \"Lecture08.html\"}, ...]\n",
    "    \n",
    "    # 3. Format the context so the LLM knows which chunk came from which lecture\n",
    "    context_with_citations = \"\"\n",
    "    for i in range(len(chunks)):\n",
    "        lecture_name = sources[i]['source']\n",
    "        context_with_citations += f\"\\n[From {lecture_name}]:\\n{chunks[i]}\\n\"\n",
    "\n",
    "    # 4. Build the prompt with instructions to cite sources\n",
    "    system_prompt = (\n",
    "        \"You are an Astronomy 1221 Tutor. Use the provided lecture snippets to answer. \"\n",
    "        \"You should always mention which lecture number you found the information in.\"\n",
    "    )\n",
    "    \n",
    "    user_message = f\"CONTEXT:\\n{context_with_citations}\\n\\nQUESTION: {query}\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    \n",
    "    # 5. Call the LLM\n",
    "    return prompt_llm(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5282c0f-540c-4fac-9d82-3a2533b190af",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = ask_astronomy_ai(\"How do we measure the distance to stars using parallax?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3ec427-643e-45f0-8bf1-62804faa3d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ask_astronomy_ai(\"What does a chunk represent in RAG?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8036d97-9662-4dcf-9ec1-06f16db7704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ask_astronomy_ai(\"What is a sentence transformer?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b74deb8-aad2-4971-a958-fc2912c9929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ask_astronomy_ai(\"What does semantic similarity mean?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ca2e87-1929-4daa-ae5c-b6f51f1abc2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
